{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.envs.registration import EnvSpec\n",
    "import numpy as np\n",
    "from multiagent.multi_discrete import MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/utamrach/opt/anaconda3/envs/ai_cs5100/lib/python3.11/site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/utamrach/opt/anaconda3/envs/ai_cs5100/lib/python3.11/site-packages (75.8.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-75.8.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: wheel in /Users/utamrach/opt/anaconda3/envs/ai_cs5100/lib/python3.11/site-packages (0.45.1)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-75.8.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.8.0\n",
      "    Uninstalling setuptools-75.8.0:\n",
      "      Successfully uninstalled setuptools-75.8.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.12.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.12.3 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorflow-estimator<2.15,>=2.14.0, but you have tensorflow-estimator 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-25.0.1 setuptools-75.8.1\n",
      "Requirement already satisfied: opencv-python in /Users/utamrach/opt/anaconda3/envs/ai_cs5100/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/utamrach/opt/anaconda3/envs/ai_cs5100/lib/python3.11/site-packages (from opencv-python) (1.23.5)\n",
      "zsh:1: no matches found: gym[atari]\n"
     ]
    }
   ],
   "source": [
    "#pip install pettingzoo\n",
    "# !pip install --upgrade pip setuptools wheel\n",
    "# !pip install opencv-python\n",
    "# !pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCitizenAgent(gym.Env):\n",
    "\n",
    "    metadata = {\n",
    "        'render.modes' : ['human', 'rgb_array']\n",
    "    }\n",
    "\n",
    "    def __init__(self, size: int = 5):\n",
    "        # The size of the square grid\n",
    "        self.size = size\n",
    "\n",
    "        # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        # Implement the logic for taking a step\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment\n",
    "        pass\n",
    "\n",
    "    def render(self, mode='human'):  \n",
    "        # Render the environment\n",
    "        pass\n",
    "\n",
    "    def step(self, action_n):\n",
    "        obs_n    = list()\n",
    "        reward_n = list()\n",
    "        done_n   = list()\n",
    "        info_n   = {'n': []}\n",
    "        # ...\n",
    "        return obs_n, reward_n, done_n, info_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    states = env.reset()  # Reset environment for each episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = [agent.act(state) for agent in agents]  # Get actions from each agent\n",
    "        next_states, rewards, done, _ = env.step(actions)  # Step the environment\n",
    "        for agent, reward in zip(agents, rewards):\n",
    "            agent.learn(state, action, reward, next_state)  # Update agent's policy\n",
    "        states = next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_cs5100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
